{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0623dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939eae2",
   "metadata": {},
   "source": [
    "### D'abord, on import les métadonnées pour les rendre exploitables par BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d084c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92393/829467055.py:1: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  meta = pd.read_csv(\"metadata.tsv\", \"\\t\") # Importation des métadonnées précédemment collectées\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_csv(\"metadata.tsv\", \"\\t\") # Importation des métadonnées précédemment collectées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f76629",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./xml/*.xml\") # On liste les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c1ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ark = [] # On isoles les id ARK propres aux fichiers présents dans le dossier\n",
    "for i in files:\n",
    "    if re.search(r\"(bpt\\w*)\", i):\n",
    "        list_ark.append(re.search(r\"(bpt\\w*)\", i).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83565e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_ark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891e8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ark in list_ark:\n",
    "\n",
    "    # Import des métadonnées pour les rendre exploitables\n",
    "    \n",
    "    df = meta.loc[meta['Ark'].str.contains(ark)]\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df2 = df.loc[df['Ark'].str.contains(ark)]\n",
    "    liste_elements = df2.iloc[0].values.flatten().tolist()\n",
    "    \n",
    "    # Création de la racine et des balises du teiHeader principales\n",
    "    \n",
    "    soup = bs.BeautifulSoup(features='xml')\n",
    "    soup.append(soup.new_tag('TEI', xmlns=\"http://www.tei-c.org/ns/1.0\"))\n",
    "    \n",
    "    soup.TEI.append(soup.new_tag('teiHeader'))\n",
    "\n",
    "    soup.TEI.teiHeader.append(soup.new_tag('fileDesc'))\n",
    "    soup.TEI.teiHeader.append(soup.new_tag('encodingDesc'))\n",
    "    soup.TEI.teiHeader.append(soup.new_tag('profileDesc'))\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.append(soup.new_tag('titleStmt'))\n",
    "    soup.TEI.teiHeader.fileDesc.append(soup.new_tag('publicationStmt'))\n",
    "    soup.TEI.teiHeader.fileDesc.append(soup.new_tag('sourceDesc'))\n",
    "\n",
    "    soup.TEI.teiHeader.encodingDesc.append(soup.new_tag('editorialDecl'))\n",
    "    soup.TEI.teiHeader.encodingDesc.append(soup.new_tag('projectDesc'))\n",
    "\n",
    "    soup.TEI.teiHeader.profileDesc.append(soup.new_tag('langUsage'))\n",
    "    \n",
    "    #On s'occupe du fileDesc\n",
    "\n",
    "        # D'abord, le titleStmt\n",
    "\n",
    "    new = soup.new_tag('title', type=\"main\")\n",
    "    new.string = \"A Muvra\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.append(new)\n",
    "\n",
    "    new = soup.new_tag('title', type=\"sub\")\n",
    "    new.string = \"Giurnale di e pieve di Corsica\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.append(new)\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.append(soup.new_tag('author'))\n",
    "    new = soup.new_tag('persName')\n",
    "    new.string = \"Petru Rocca\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.author.append(new)\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.append(soup.new_tag('respStmt'))\n",
    "    new = soup.new_tag('name')\n",
    "    new.string = \"Deborah Paci\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "    new = soup.new_tag('orgName')\n",
    "    new.string = \"Università di Bologna | Università di Modena e Reggio Emilia\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "    new = soup.new_tag('resp')\n",
    "    new.string = \"Supervision et inteprétation des résultats\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.insert(3 ,soup.new_tag('respStmt'))\n",
    "    new = soup.new_tag('name')\n",
    "    new.string = \"Vincent Sarbach-Pulicani\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "    new = soup.new_tag('orgName')\n",
    "    new.string = \"École nationale des chartes - PSL | Università di Pisa\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "    new = soup.new_tag('resp')\n",
    "    new.string = \"Encodage TEI, récolte et exploitation des données\"\n",
    "    soup.TEI.teiHeader.fileDesc.titleStmt.respStmt.append(new)\n",
    "\n",
    "        # Ensuite, le publicationStmt\n",
    "\n",
    "    new = soup.new_tag('distributor')\n",
    "    new.string = \"BnF (Bibliothèque nationale de France)\"\n",
    "    soup.TEI.teiHeader.fileDesc.publicationStmt.append(new)\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.publicationStmt.append(soup.new_tag(\"address\"))\n",
    "    list_str = ['Bibliothèque François Mitterand', 'Quai François Mauriac', '75706, Paris', 'France']\n",
    "    for s in list_str:\n",
    "        new = soup.new_tag('addrLine')\n",
    "        new.string = s\n",
    "        soup.TEI.teiHeader.fileDesc.publicationStmt.address.append(new)\n",
    "\n",
    "    new = soup.new_tag('idno', type=\"ark\")\n",
    "    new.string = liste_elements[0]\n",
    "    soup.TEI.teiHeader.fileDesc.publicationStmt.append(new)\n",
    "\n",
    "    new = soup.new_tag('availability', status=\"free\")\n",
    "    soup.TEI.teiHeader.fileDesc.publicationStmt.append(new)\n",
    "\n",
    "    new = soup.new_tag('p')\n",
    "    new.string = liste_elements[4]\n",
    "    soup.TEI.teiHeader.fileDesc.publicationStmt.availability.append(new)\n",
    "\n",
    "\n",
    "        # Enfin, le sourceDesc\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.append(soup.new_tag('bibl'))\n",
    "\n",
    "    new = soup.new_tag('author')\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "\n",
    "    new = soup.new_tag('persName')\n",
    "    new.string = \"Petru Rocca\"\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.author.append(new)\n",
    "\n",
    "    new = soup.new_tag('title', type=\"main\")\n",
    "    new.string = \"A Muvra\"\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "\n",
    "    new = soup.new_tag('title', type=\"sub\")\n",
    "    new.string = \"Giurnale di e pieve di Corsica\"\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "\n",
    "    new = soup.new_tag('distributor')\n",
    "    new.string = \"Stamparia di A Muvra\"\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(soup.new_tag(\"address\"))\n",
    "    list_str = ['38 Cours Grandval', 'Aiacciu, Corsica', 'France']\n",
    "    for s in list_str:\n",
    "        new = soup.new_tag('addrLine')\n",
    "        new.string = s\n",
    "        soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.address.append(new)\n",
    "\n",
    "    new = soup.new_tag('edition')\n",
    "    new.string = \"Numéro \" + str(liste_elements[3]) + \" | Année \" + str(liste_elements[2])\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "\n",
    "    new = soup.new_tag('date')\n",
    "    new.string = str(liste_elements[1])\n",
    "    soup.TEI.teiHeader.fileDesc.sourceDesc.bibl.append(new)\n",
    "    \n",
    "    # On passe maintenant à l'encodingDesc\n",
    "\n",
    "    soup.TEI.teiHeader.encodingDesc.editorialDecl.append(soup.new_tag('normalization'))\n",
    "\n",
    "    new = soup.new_tag('p')\n",
    "    new.string = \"Le texte a été mis en minuscule. Normalisation des césures selon le contexte  et suppression des tirets en fin de ligne pour homogénéiser les mots. La ponctuation a également été supprimée pour diminuer les conséquences du bruit de l'OCR.\"\n",
    "    soup.TEI.teiHeader.encodingDesc.editorialDecl.normalization.append(new)\n",
    "\n",
    "    new = soup.new_tag('p')\n",
    "    new.string = \"Utilisation de Tesseract OCR (cos + fra + ita). Documents segmentés à partir de fichiers UZN obtenus grâce aux fichiers ALTO mis à disposition par Gallica. Il s'agit de la publication des transcriptions des numéros d'« A Muvra » disponibles sur la plateforme Gallica de la BnF. Ces documents servent d'abord au mémoire de recherche en « Humanités numériques » de Vincent Sarbach-Pulicani. Ils servent également au projet d'article pour la revue « Umanistica digitale », issu d'une collaboration entre Deborah Paci et Vincent Sarbach-Pulicani.\"\n",
    "    soup.TEI.teiHeader.encodingDesc.projectDesc.append(new)\n",
    "    \n",
    "    # Enfin, on passe au profileDesc\n",
    "\n",
    "    list_str = [\"cos\", \"fra\", \"ita\"]\n",
    "    for i in list_str:\n",
    "        new = soup.new_tag('language', ident=i)\n",
    "        soup.TEI.teiHeader.profileDesc.langUsage.append(new)   \n",
    "\n",
    "    # On crée d'abord le text puis le body\n",
    "\n",
    "    soup.TEI.append(soup.new_tag(\"blabla\"))\n",
    "    soup.TEI.blabla.append(soup.new_tag(\"body\"))\n",
    "\n",
    "    # Importation du fichier xml sous format dataframe\n",
    "\n",
    "    df3 = pd.read_xml(\"./xml/\" + ark + \".xml\")\n",
    "    df3 = df3.drop('lan', axis=1)\n",
    "    df3 = df3.drop('regio', axis=1)\n",
    "    df3 = df3.drop('typ', axis=1)\n",
    "\n",
    "    # On incorpore les différentes rubriques sous les balises <div>\n",
    "\n",
    "    for i in reversed(range(1, len(df3.index))):\n",
    "        list_revue = df3.iloc[i].values.flatten().tolist()\n",
    "\n",
    "        new = soup.new_tag('div', type=str(list_revue[4]),  n=str(list_revue[0]))\n",
    "        soup.TEI.blabla.body.insert(0,new)\n",
    "\n",
    "        new = soup.new_tag('p')\n",
    "        new.string = str(list_revue[1])\n",
    "        soup.TEI.blabla.body.div.append(new)\n",
    "\n",
    "        new = soup.new_tag('docAuthor')\n",
    "        new.string = str(list_revue[3])\n",
    "        soup.TEI.blabla.body.div.append(new)\n",
    "\n",
    "        soup.TEI.blabla.body.find('div')['xml:lang'] = str(list_revue[2])\n",
    "\n",
    "    #On fait maintenant la manchette\n",
    "\n",
    "    list_revue = df3.iloc[0].values.flatten().tolist()\n",
    "\n",
    "    new = soup.new_tag('div', type='manchette',  n=str(list_revue[0]))\n",
    "    soup.TEI.blabla.body.insert(0,new)\n",
    "\n",
    "    new = soup.new_tag('p')\n",
    "    new.string = str(list_revue[1])\n",
    "    soup.TEI.blabla.body.div.append(new)\n",
    "\n",
    "    new = soup.new_tag('docAuthor')\n",
    "    new.string = 'Inconnu'\n",
    "    soup.TEI.blabla.body.div.append(new)\n",
    "\n",
    "    soup.TEI.blabla.body.find('div')['xml:lang'] = 'co'\n",
    "\n",
    "    tag_bla = soup.find('blabla')\n",
    "    tag_bla.name = 'text'\n",
    "\n",
    "    # pour finir, on crée un fichier dans le dossier tei/\n",
    "    \n",
    "    filename = \"AMuvra_N\" + str(liste_elements[3]) + \"_A\" + str(liste_elements[2]) + \"_\" + ark + \".xml\"\n",
    "\n",
    "    with open(\"./tei/\" + filename, \"w\") as f:\n",
    "        f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fefb65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0c6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
