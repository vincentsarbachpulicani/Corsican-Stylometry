{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6aff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from lxml import etree\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import stopwords_processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab8d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./xml/*.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be650097",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fonctions basiques\n",
    "\n",
    "### Language selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b56cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_selection(language):\n",
    "    if language == 'co':\n",
    "        name = 'corsican'\n",
    "        path = '//rubrique[@lan=\"co\"]/'\n",
    "    elif language == 'fr':\n",
    "        name = 'french'\n",
    "        path = '//rubrique[@lan=\"fr\"]/'\n",
    "    elif language == 'it':\n",
    "        name = 'italian'\n",
    "        path = '//rubrique[@lan=\"it\"]/'\n",
    "    else:\n",
    "        raise ValueError('Wrong language identifier. Try with \"co\", \"fr\" or \"it\".')\n",
    "        \n",
    "    return name, path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0d695",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Extracting data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867e4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(list_files, path):\n",
    "    all_textes = []\n",
    "    all_arks = []\n",
    "    all_auteurs = []\n",
    "    all_typ = []\n",
    "    all_index = []\n",
    "\n",
    "    for xml in list_files:\n",
    "\n",
    "        if re.search(r\"(bpt\\w*)\", xml):\n",
    "            ark = re.search(r\"(bpt\\w*)\", xml).group(1)\n",
    "\n",
    "        tree = etree.parse(xml)\n",
    "\n",
    "        arks = []\n",
    "        textes = []\n",
    "        index = []\n",
    "        typ = []\n",
    "        auteur = []\n",
    "\n",
    "        for tag in tree.xpath(path + 'texte'):\n",
    "            x = tag.text\n",
    "            x = re.sub(r'\\n', r'', x)\n",
    "            textes.append(x)\n",
    "\n",
    "            arks.append(ark)\n",
    "\n",
    "        for tag in tree.xpath(path + 'auteur'):\n",
    "            x = tag.text\n",
    "            x = re.sub(r'\\n', r'', x)\n",
    "            auteur.append(x)\n",
    "\n",
    "        for tag in tree.xpath(path + 'type'):\n",
    "            x = tag.text\n",
    "            x = re.sub(r'\\n', r'', x)\n",
    "            typ.append(x)\n",
    "\n",
    "        for tag in tree.xpath(path + 'index'):\n",
    "            x = tag.text\n",
    "            x = re.sub(r'\\n', r'', x)\n",
    "            index.append(x)\n",
    "\n",
    "        all_textes.extend(textes)\n",
    "        all_auteurs.extend(auteur)\n",
    "        all_index.extend(index)\n",
    "        all_typ.extend(typ)\n",
    "        all_arks.extend(arks)\n",
    "        \n",
    "    dic = {\"Texts\":all_textes, \"Auteurs\":all_auteurs, \"Type\":all_typ, \"Position\":all_index, \"Arks\":all_arks}\n",
    "    df = pd.DataFrame(dic) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a69d4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hapax et duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2942ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapax_counter(freq_tokens):\n",
    "    list_hapax = []\n",
    "    for k, v in freq_tokens.items():\n",
    "        if 1 == v:\n",
    "            list_hapax.append(k)\n",
    "    return list_hapax\n",
    "\n",
    "def duplicates_counter(freq_tokens):    \n",
    "    duplicates = []\n",
    "    for k, v in freq_tokens.items():\n",
    "        if 1 < v:\n",
    "            duplicates.append(k)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8501a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fonctions de création de datasets\n",
    "\n",
    "### Dataset entier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e1dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dataset_selection(list_files, language, rem_stopwords = False):\n",
    "    \n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(files, path)\n",
    "        \n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        df.to_csv(\"./Full/full_\" + name + \"_dataset.tsv\", '\\t')\n",
    "        print(\"full_\" + name + \"_dataset.tsv has been created with success.\")\n",
    "    else:\n",
    "        df = stopwords_processing.delete_stopwords_from_dataframe(table=df, col_name=\"Texts\")\n",
    "        df.to_csv(\"./Full/full_\" + name + \"_withoutSW_dataset.tsv\", '\\t')\n",
    "        print(\"full_\" + name + \"_withoutSW_dataset.tsv has been created with success.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f48869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_french_dataset.tsv has been created with success.\n",
      "full_corsican_dataset.tsv has been created with success.\n",
      "full_italian_dataset.tsv has been created with success.\n"
     ]
    }
   ],
   "source": [
    "full_dataset_selection(files, 'fr')\n",
    "full_dataset_selection(files, 'co')\n",
    "full_dataset_selection(files, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2ebcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object -> dataframe.\n",
      "Done.\n",
      "full_corsican_withoutSW_dataset.tsv has been created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "full_french_withoutSW_dataset.tsv has been created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "full_italian_withoutSW_dataset.tsv has been created with success.\n"
     ]
    }
   ],
   "source": [
    "full_dataset_selection(files, 'co', rem_stopwords=True)\n",
    "full_dataset_selection(files, 'fr', rem_stopwords=True)\n",
    "full_dataset_selection(files, 'it', rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2eec9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Datasets d'entraînements et de tests pour le Corse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a33ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_datasets(list_files, language, threshold, rem_stopwords = False):\n",
    "    \n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(files, path)       \n",
    "    \n",
    "    df_test = df.iloc[:threshold]\n",
    "    df_test = pd.DataFrame(df_test)\n",
    "    \n",
    "    df_train = df.iloc[threshold + 1:]\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        df_test.to_csv(\"./Test/test_\" + name + \"_dataset.tsv\", '\\t')\n",
    "        df_train.to_csv(\"./Train/train_\" + name + \"_dataset.tsv\", '\\t')\n",
    "        \n",
    "        print(\"Train and test dataset for \" + name + \" created with success.\")\n",
    "    else:\n",
    "        df_test = stopwords_processing.delete_stopwords_from_dataframe(df_test, col_name=\"Texts\")\n",
    "        df_train = stopwords_processing.delete_stopwords_from_dataframe(df_train, col_name=\"Texts\")\n",
    "        \n",
    "        df_test.to_csv(\"./Test/test_\" + name + \"_withoutSW_dataset.tsv\", '\\t')\n",
    "        df_train.to_csv(\"./Train/train_\" + name + \"_withoutSW_dataset.tsv\", '\\t')\n",
    "        \n",
    "        print(\"Train and test dataset for \" + name + \", without stopwords, created with success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8e5337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset for corsican created with success.\n",
      "Train and test dataset for french created with success.\n",
      "Train and test dataset for italian created with success.\n"
     ]
    }
   ],
   "source": [
    "train_test_datasets(files, 'co', 400)\n",
    "train_test_datasets(files, 'fr', 90)\n",
    "train_test_datasets(files, 'it', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81d42d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object -> dataframe.\n",
      "Done.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Train and test dataset for corsican, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Train and test dataset for french, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Train and test dataset for italian, without stopwords, created with success.\n"
     ]
    }
   ],
   "source": [
    "train_test_datasets(files, 'co', 400, rem_stopwords=True)\n",
    "train_test_datasets(files, 'fr', 90, rem_stopwords=True)\n",
    "train_test_datasets(files, 'it', 70, rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67caa9e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Fréquences de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(list_files, language, rem_stopwords=False):\n",
    "\n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(files, path)\n",
    "    \n",
    "    if rem_stopwords is True:\n",
    "        df = stopwords_processing.delete_stopwords_from_dataframe(table=df, col_name=\"Texts\")\n",
    "    \n",
    "    liste = df['Texts'].tolist()\n",
    "    corpus = ' '.join(liste)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "    frequence = dict(nltk.FreqDist(tokens))\n",
    "    \n",
    "    df = pd.DataFrame(frequence, index=[\"Frequency\"])\n",
    "    df.index.name = \"Word\"\n",
    "    df = df.T\n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        df.to_csv(\"./Frequency/WordFrequency_\" + name + \"_dataset.tsv\", '\\t')\n",
    "        print(\"WordFrequency_\" + name + \"_dataset.tsv created with success.\")\n",
    "    else:\n",
    "        df.to_csv(\"./Frequency/WordFrequency_\" + name + \"_withoutSW_dataset.tsv\", '\\t')\n",
    "        print(\"WordFrequency_\" + name + \"_withoutSW_dataset.tsv created with success.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28dbdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordFrequency_corsican_dataset.tsv created with success.\n",
      "WordFrequency_french_dataset.tsv created with success.\n",
      "WordFrequency_italian_dataset.tsv created with success.\n"
     ]
    }
   ],
   "source": [
    "word_frequency(files, 'co')\n",
    "word_frequency(files, 'fr')\n",
    "word_frequency(files, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b6bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object -> dataframe.\n",
      "Done.\n",
      "WordFrequency_corsican_withoutSW_dataset.tsv created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "WordFrequency_french_withoutSW_dataset.tsv created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "WordFrequency_italian_withoutSW_dataset.tsv created with success.\n"
     ]
    }
   ],
   "source": [
    "word_frequency(files, 'co', rem_stopwords=True)\n",
    "word_frequency(files, 'fr', rem_stopwords=True)\n",
    "word_frequency(files, 'it', rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01255ab2",
   "metadata": {},
   "source": [
    "---\n",
    "### Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0b103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_counter(list_files, language, rem_stopwords=False):\n",
    "\n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(files, path)\n",
    "    \n",
    "    if rem_stopwords is True:\n",
    "        df = stopwords_processing.delete_stopwords_from_dataframe(table=df, col_name=\"Texts\")\n",
    "    \n",
    "    liste = df['Texts'].tolist()\n",
    "    corpus = ' '.join(liste)\n",
    "\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "    frequence = dict(nltk.FreqDist(tokens))\n",
    "    hapax = hapax_counter(frequence)\n",
    "    duplicates = duplicates_counter(frequence)\n",
    "    vocabulary = duplicates + hapax\n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        with open(\"./Vocabulary/vocabulary_\" + name + \".txt\", \"w\") as f:\n",
    "            for word in vocabulary:\n",
    "                f.write(word + '\\n')\n",
    "        print(\"Vocabulary for \" + name + \" created with success.\")\n",
    "    else:\n",
    "        with open(\"./Vocabulary/vocabulary_\" + name + \"_withoutSW.txt\", \"w\") as f:\n",
    "            for word in vocabulary:\n",
    "                f.write(word + '\\n')\n",
    "        print(\"Vocabulary for \" + name + \", without stopwords, created with success.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96f6bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary for corsican created with success.\n",
      "Vocabulary for french created with success.\n",
      "Vocabulary for italian created with success.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_counter(files, 'co')\n",
    "vocabulary_counter(files, 'fr')\n",
    "vocabulary_counter(files, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ddda6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object -> dataframe.\n",
      "Done.\n",
      "Vocabulary for corsican, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Vocabulary for french, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Vocabulary for italian, without stopwords, created with success.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_counter(files, 'co', rem_stopwords=True)\n",
    "vocabulary_counter(files, 'fr', rem_stopwords=True)\n",
    "vocabulary_counter(files, 'it', rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065b16f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vocabulaire avec suppression des hapax et mots peu fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059da54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfw_vocabulary(list_files, language, threshold, rem_stopwords = False):\n",
    "    \n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(files, path)\n",
    "    \n",
    "    if rem_stopwords is True:\n",
    "        df = stopwords_processing.delete_stopwords_from_dataframe(table=df, col_name=\"Texts\")\n",
    "    \n",
    "    liste = df['Texts'].tolist()\n",
    "    corpus = ' '.join(liste)\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "    tokens = tokenizer.tokenize(corpus)\n",
    "    \n",
    "    freqdist = dict(nltk.FreqDist(tokens))\n",
    "    mfw = {key:val for key, val in freqdist.items() if val >= threshold}\n",
    "    print(\"Le vocabulaire restreint contient dorénavant \" + str(len(mfw)) + \" au lieu de \" + str(len(freqdist)))\n",
    " \n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        with open(\"./MFW/MFWVocabulary_\" + str(threshold) + name + \".txt\", \"w\") as f:\n",
    "            for word in mfw:\n",
    "                f.write(word + '\\n')\n",
    "        print(\"Vocabulary for \" + name + \" created with success.\")\n",
    "    else:\n",
    "        with open(\"./MFW/MFWVocabulary_\" + str(threshold) + name + \"_withoutSW.txt\", \"w\") as f:\n",
    "            for word in mfw:\n",
    "                f.write(word + '\\n')\n",
    "        print(\"Vocabulary for \" + name + \", without stopwords, created with success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a37290f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le vocabulaire restreint contient dorénavant 7154 au lieu de 86825\n",
      "Vocabulary for corsican created with success.\n",
      "Le vocabulaire restreint contient dorénavant 3593 au lieu de 40625\n",
      "Vocabulary for french created with success.\n",
      "Le vocabulaire restreint contient dorénavant 2677 au lieu de 38927\n",
      "Vocabulary for italian created with success.\n"
     ]
    }
   ],
   "source": [
    "mfw_vocabulary(files, 'co', threshold=10)\n",
    "mfw_vocabulary(files, 'fr', threshold=10)\n",
    "mfw_vocabulary(files, 'it', threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98c950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object -> dataframe.\n",
      "Done.\n",
      "Le vocabulaire restreint contient dorénavant 6764 au lieu de 86093\n",
      "Vocabulary for corsican, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Le vocabulaire restreint contient dorénavant 3189 au lieu de 40001\n",
      "Vocabulary for french, without stopwords, created with success.\n",
      "Object -> dataframe.\n",
      "Done.\n",
      "Le vocabulaire restreint contient dorénavant 2226 au lieu de 38227\n",
      "Vocabulary for italian, without stopwords, created with success.\n"
     ]
    }
   ],
   "source": [
    "mfw_vocabulary(files, 'co', threshold=10, rem_stopwords=True)\n",
    "mfw_vocabulary(files, 'fr', threshold=10, rem_stopwords=True)\n",
    "mfw_vocabulary(files, 'it', threshold=10, rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3121a",
   "metadata": {},
   "source": [
    "---\n",
    "### Document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d1ad69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(vocabulary, list_files, language, rem_stopwords=False):\n",
    "\n",
    "    name, path = language_selection(language)\n",
    "    \n",
    "    df = data_extraction(list_files, path)\n",
    "    \n",
    "    corpus = df['Texts'].tolist()\n",
    "    \n",
    "    with open(vocabulary, 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform(vocab)\n",
    "    td = vectorizer.transform(corpus)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1 = pd.DataFrame(td.todense())\n",
    "    df1.columns = vectorizer.get_feature_names()\n",
    "    term_document_matrix = df1.T\n",
    "    \n",
    "    \n",
    "    if rem_stopwords is False:\n",
    "        term_document_matrix.to_csv(\"./DocumentTermMatrix/DocumentTermMatrix_\" + name + \"_dataset.tsv\", '\\t')\n",
    "        print(\"DocumentTermMatrix_\" + name + \"_dataset.tsv created with success.\")\n",
    "    else:\n",
    "        term_document_matrix.to_csv(\"./DocumentTermMatrix/DocumentTermMatrix_\" + name + \"_withoutSW_dataset.tsv\", '\\t')\n",
    "        print(\"DocumentTermMatrix_\" + name + \"_withoutSW_dataset.tsv created with success.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c5355c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentTermMatrix_corsican_dataset.tsv created with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentTermMatrix_french_dataset.tsv created with success.\n",
      "DocumentTermMatrix_italian_dataset.tsv created with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "document_term_matrix(\"./MFW/MFWVocabulary_10corsican.txt\", files, 'co')\n",
    "document_term_matrix(\"./MFW/MFWVocabulary_10french.txt\", files, 'fr')\n",
    "document_term_matrix(\"./MFW/MFWVocabulary_10italian.txt\", files, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa2f9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentTermMatrix_corsican_withoutSW_dataset.tsv created with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentTermMatrix_french_withoutSW_dataset.tsv created with success.\n",
      "DocumentTermMatrix_italian_withoutSW_dataset.tsv created with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/hn/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "document_term_matrix(\"./MFW/MFWVocabulary_10corsican_withoutSW.txt\", files, 'co', rem_stopwords=True)\n",
    "document_term_matrix(\"./MFW/MFWVocabulary_10french_withoutSW.txt\", files, 'fr', rem_stopwords=True)\n",
    "document_term_matrix(\"./MFW/MFWVocabulary_10italian_withoutSW.txt\", files, 'it', rem_stopwords=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
